{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jitender Singh Virk ||\n",
    "Date created: 3 Jan 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tf_utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset directory and files\n",
    "#data_location = \"C:\\\\Users\\\\Virksaab\\\\Desktop\\\\AI\\\\datasets_for_code_in_repos\\\\char74k_dataset\\\\\"\n",
    "data_location = \"/home/virk/Desktop/AI/datasets_for_code_in_repos/char74k_dataset/\"\n",
    "TRAIN_FILE = \"train.tfrecords\"\n",
    "VALIDATE_FILE = \"validate.tfrecords\"\n",
    "TEST_FILE = \"test.tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global constants describing the data set.\n",
    "NUM_CLASSES = 62\n",
    "IMAGE_SIZE = 64\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE * 3\n",
    "TRAIN_SET_SIZE = 8704\n",
    "VALIDATE_SET_SIZE = 1792\n",
    "TEST_SET_SIZE = 1792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HyperParameters for traning\n",
    "neurons_in_layer = [IMAGE_PIXELS, 1984, 992, 496, 248, 124, NUM_CLASSES]\n",
    "num_epochs = 20\n",
    "bs = 1792\n",
    "train_batch_size = 8704\n",
    "val_batch_size = bs\n",
    "test_batch_size = bs\n",
    "iterations = 1000\n",
    "learning_rate = 0.0001 # Decaying if start_\n",
    "L2_beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_graph(batch_size, is_training):\n",
    "    # READ DATA\n",
    "    # Read training data from tfrecords file\n",
    "    train_x_batch, train_y_batch = tf_utils.read_images_from_tfrecords_in_shuffle_batch(data_location+TRAIN_FILE, image_shape=IMAGE_PIXELS,\n",
    "                                                                                        batch_size=train_batch_size, num_epochs=num_epochs, shuffle=True)\n",
    "    train_y_batch = tf.one_hot(train_y_batch, NUM_CLASSES)\n",
    "    #train_x_batch, train_y_batch = tf.transpose(train_x_batch), tf.transpose(train_y_batch)\n",
    "\n",
    "    # Read validation data from tfrecords file\n",
    "    val_x_batch, val_y_batch = tf_utils.read_images_from_tfrecords_in_shuffle_batch(data_location+VALIDATE_FILE, image_shape=IMAGE_PIXELS,\n",
    "                                                                                    batch_size=val_batch_size, num_epochs=1, shuffle=True)\n",
    "    val_y_batch = tf.one_hot(val_y_batch, NUM_CLASSES)\n",
    "    #val_x_batch, val_y_batch = tf.transpose(val_x_batch), tf.transpose(val_y_batch)\n",
    "\n",
    "    # Read testing data from tfrecords file\n",
    "    test_x_batch, test_y_batch = tf_utils.read_images_from_tfrecords_in_shuffle_batch(data_location+TEST_FILE, image_shape=IMAGE_PIXELS,\n",
    "                                                                                    batch_size=test_batch_size, num_epochs=1, shuffle=True)\n",
    "    test_y_batch = tf.one_hot(test_y_batch, NUM_CLASSES)\n",
    "    #test_x_batch, test_y_batch = tf.transpose(test_x_batch), tf.transpose(test_y_batch)\n",
    "    \n",
    "    # PLACEHOLDERS\n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size, IMAGE_PIXELS])\n",
    "    Y = tf.placeholder(tf.float32, shape=[batch_size, NUM_CLASSES])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    # LAYERS\n",
    "    ## Layer-1\n",
    "    w1 = tf.get_variable(\"w1\", [neurons_in_layer[1], neurons_in_layer[0]], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    z1 = tf.matmul(w1, tf.transpose(X))\n",
    "    z1_bn, bn_cache1 = tf_utils.batch_norm_wrapper(1, z1, is_training=is_training)\n",
    "    a1 = tf.nn.relu(z1_bn)\n",
    "    a1_dropout = tf.nn.dropout(a1, keep_prob)\n",
    "    ## Layer-2\n",
    "    w2 = tf.get_variable(\"w2\", [neurons_in_layer[2], neurons_in_layer[1]], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    z2 = tf.matmul(w2, a1_dropout)\n",
    "    z2_bn, bn_cache2 = tf_utils.batch_norm_wrapper(2, z2, is_training=is_training)\n",
    "    a2 = tf.nn.relu(z2_bn)\n",
    "    a2_dropout = tf.nn.dropout(a2, keep_prob)\n",
    "    ## Layer-3\n",
    "    w3 = tf.get_variable(\"w3\", [neurons_in_layer[3], neurons_in_layer[2]], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    z3 = tf.matmul(w3, a2_dropout)\n",
    "    z3_bn, bn_cache3 = tf_utils.batch_norm_wrapper(3, z3, is_training=is_training)\n",
    "    a3 = tf.nn.relu(z3_bn)\n",
    "    a3_dropout = tf.nn.dropout(a3, keep_prob)\n",
    "    ## Layer-4\n",
    "    w4 = tf.get_variable(\"w4\", [neurons_in_layer[4], neurons_in_layer[3]], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    z4 = tf.matmul(w4, a3_dropout)\n",
    "    z4_bn, bn_cache4 = tf_utils.batch_norm_wrapper(4, z4, is_training=is_training)\n",
    "    a4 = tf.nn.relu(z4_bn)\n",
    "    a4_dropout = tf.nn.dropout(a4, keep_prob)\n",
    "    ## Layer-5\n",
    "    w5 = tf.get_variable(\"w5\", [neurons_in_layer[5], neurons_in_layer[4]], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    z5 = tf.matmul(w5, a4_dropout)\n",
    "    z5_bn, bn_cache5 = tf_utils.batch_norm_wrapper(5, z5, is_training=is_training)\n",
    "    a5 = tf.nn.relu(z5_bn)\n",
    "    a5_dropout = tf.nn.dropout(a5, keep_prob)\n",
    "    ## Layer-6\n",
    "    w6 = tf.get_variable(\"w6\", [neurons_in_layer[6], neurons_in_layer[5]], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    z6 = tf.matmul(w6, a5_dropout)\n",
    "    z6_bn, bn_cache6 = tf_utils.batch_norm_wrapper(6, z6, is_training=is_training)\n",
    "\n",
    "    Yhat = tf.transpose(z6_bn)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Yhat, labels=Y))\n",
    "    # Loss function with L2 Regularization with decaying learning rate\n",
    "    regularizers = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3) + tf.nn.l2_loss(w4) + tf.nn.l2_loss(w5) + tf.nn.l2_loss(w6)           \n",
    "    loss = tf.reduce_mean(loss + L2_beta * regularizers)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # save your variables for later use\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if is_training:\n",
    "        return (train_x_batch, train_y_batch), (X, Y, keep_prob), optimizer, loss, Yhat, saver, bn_cache1\n",
    "    else:\n",
    "        return (val_x_batch, val_y_batch, test_x_batch, test_y_batch), (X, Y, keep_prob), Yhat, saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build training graph, train and save the trained model\n",
    "tf.reset_default_graph()\n",
    "(train_x_batch, train_y_batch), (X, Y, keep_prob), optimizer, loss, Yhat, saver, bn_cache = build_graph(train_batch_size, is_training=True)\n",
    "\n",
    "# Start Session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Number of batches in training data   \n",
    "num_batches = math.ceil(TRAIN_SET_SIZE / train_batch_size)\n",
    "\n",
    "# Initialiize Variables\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "# Set coordinator and start queue runner for training set\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)  \n",
    " \n",
    "# TRAINING\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    print(\"EPOCH {}/{}\".format(epoch, num_epochs), \"-\"*40)\n",
    "    for batch_no in range(1, (num_batches+1)):\n",
    "        try:\n",
    "            train_X, train_Y = sess.run([train_x_batch, train_y_batch])\n",
    "            for iteration in range(1, iterations+1):\n",
    "                print(iteration)\n",
    "                _, batch_loss = sess.run([optimizer, loss], {X:train_X, Y:train_Y, keep_prob:0.5})\n",
    "            print(\"loss of batch no. {}/{} after optimizing with {} iters is {}\".format(batch_no, num_batches, iterations, batch_loss))\n",
    "    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('TRAIN batch limit reached')\n",
    "            \n",
    "        #print(sess.run([tf.argmax(tf.nn.softmax(Yhat), 1), tf.argmax(Y, 1)], {X:train_X, Y:train_Y, keep_prob:1.0}))\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.nn.softmax(Yhat), 1), tf.argmax(Y, 1)), \"float\")) * 100.\n",
    "        print(\"Train accuracy(per batch) =\", sess.run(acc, {X:train_X, Y:train_Y, keep_prob:1.0}))\n",
    "    \n",
    "sc, bt, pm, pv = sess.run(bn_cache)\n",
    "#print(sc.shape, bt.shape, pm.shape, pv.shape)\n",
    "\n",
    "# Save your training\n",
    "save_path = saver.save(sess, \"./char74k_training/training\")\n",
    "\n",
    "# Wait for training threads to finish.\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "# Don't forget to close session\n",
    "sess.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build Validate and testing graph\n",
    "tf.reset_default_graph()\n",
    "(val_x_batch, val_y_batch, test_x_batch, test_y_batch), (X, Y, keep_prob), Yhat, saver = build_graph(val_batch_size, is_training=False)\n",
    "\n",
    "# Start Session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Number of batches in validate data   \n",
    "num_val_batches = math.ceil(VALIDATE_SET_SIZE / val_batch_size)\n",
    "# Number of batches in testing data\n",
    "num_test_batches = math.ceil(TEST_SET_SIZE / test_batch_size)\n",
    "\n",
    "# Initialiize Variables\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess.run(init_op)\n",
    "\n",
    "# Restore saved training variables\n",
    "saver.restore(sess, \"./char74k_training/training\")\n",
    "\n",
    "# Set coordinator and start queue runner for training set\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord) \n",
    "\n",
    "# VALIDATE\n",
    "for batch_no in range(1, (num_val_batches+1)):\n",
    "        try:\n",
    "            val_X, val_Y = sess.run([val_x_batch, val_y_batch])\n",
    "\n",
    "             # Calculate accuracy on the validate set\n",
    "            val_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.nn.softmax(Yhat), 1), tf.argmax(Y, 1)), \"float\")) * 100.\n",
    "            print(\"Validate Accuracy of batch {} is {}\".format(batch_no, sess.run(val_accuracy, {X: val_X, Y: val_Y, keep_prob:1.0})))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('VALIDATE batch limit reached')\n",
    "\n",
    "#print(sess.run([tf.argmax(tf.nn.softmax(Yhat), 1), tf.argmax(Y, 1)], {X: val_X, Y: val_Y, keep_prob:1.0}))            \n",
    "# TESTING\n",
    "for batch_no in range(1, (num_test_batches+1)):\n",
    "        try:\n",
    "            test_X, test_Y = sess.run([test_x_batch, test_y_batch])\n",
    "\n",
    "             # Calculate accuracy on the test set\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(tf.nn.softmax(Yhat), 1), tf.argmax(Y, 1)), \"float\")) * 100.\n",
    "            print(\"Testing Accuracy of batch {} is {}\".format(batch_no, sess.run(test_accuracy, {X: test_X, Y: test_Y, keep_prob:1.0})))\n",
    "            \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('TEST batch limit reached')\n",
    "\n",
    "            \n",
    "# Wait for training threads to finish.\n",
    "coord.request_stop()\n",
    "coord.join(threads)\n",
    "# Don't forget to close session\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2018, Jitender Singh Virk\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
